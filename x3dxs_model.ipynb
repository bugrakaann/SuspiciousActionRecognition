{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bugrakaann/SuspiciousActionRecognition/blob/master/x3dxs_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorchvideo torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFgv3OC86_1G",
        "outputId": "0dc8a2e7-99d7-4038-c4cd-d9b97ee3d4be"
      },
      "id": "WFgv3OC86_1G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorchvideo\n",
            "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/132.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n",
            "Collecting fvcore (from pytorchvideo)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av (from pytorchvideo)\n",
            "  Downloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting parameterized (from pytorchvideo)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting iopath (from pytorchvideo)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m768.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.5.1+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (2.5.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Collecting portalocker (from iopath->pytorchvideo)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188685 sha256=da3213bb6eb416818e4209468acad27dc1b87e1e4f7fe08972eedc50c2861eb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/6d/ae/d016375a73be141a0e11bb42289e2d0b046c35687fc8010ecc\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61396 sha256=9fde0248a0f7f51c54708d0f0eeaa4b2d91f498e52e7ad7bc3a49123f4d3e3b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=10b401c6534aecad82acef5e6e3e6e577fd2643bf4927ff9830b2e262358f2cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, parameterized, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, av, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, nvidia-cusolver-cu12, fvcore, pytorchvideo\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T11:46:22.236707Z",
          "start_time": "2025-02-22T11:46:16.336297Z"
        },
        "id": "aea6d8027b7de87f"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ],
      "id": "aea6d8027b7de87f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T12:45:17.298142Z",
          "start_time": "2025-02-22T12:45:16.394712Z"
        },
        "id": "d450b013940b2438"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/dataset.zip\" -d \"/content/sample_data\""
      ],
      "id": "d450b013940b2438",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T12:45:17.315483800Z",
          "start_time": "2025-02-22T11:46:22.239686Z"
        },
        "id": "8be6ceab98346e92"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# üî• Ana dataset klas√∂r√º\n",
        "dataset_path = \"/content/sample_data/dataset/Videos/Videos/\"\n",
        "\n",
        "# üìå Normal ve Abnormal klas√∂rlerini al\n",
        "normal_path = os.path.normpath(os.path.join(dataset_path, \"normal\"))\n",
        "abnormal_path = os.path.normpath(os.path.join(dataset_path, \"abnormal\"))\n",
        "\n",
        "# üìå T√ºm abnormal sƒ±nƒ±flarƒ± al\n",
        "abnormal_classes = [folder for folder in os.listdir(abnormal_path) if os.path.isdir(os.path.join(abnormal_path, folder))]\n",
        "class_to_index = {cls: idx for idx, cls in enumerate(abnormal_classes, start=1)}  # 1, 2, 3... (Abnormal sƒ±nƒ±flar)\n",
        "\n",
        "# üìå DataFrame olu≈üturmak i√ßin liste\n",
        "data = []\n",
        "\n",
        "# ‚úÖ Normal videolar (Binary = 0, Multi = -1)\n",
        "for subfolder in os.listdir(normal_path):\n",
        "    subfolder_path = os.path.normpath(os.path.join(normal_path, subfolder))\n",
        "\n",
        "    if os.path.isdir(subfolder_path):  # Eƒüer bir klas√∂rse\n",
        "        videos = [\n",
        "            os.path.normpath(os.path.join(subfolder_path, v)).replace(\"\\\\\", \"/\")  # Windows uyumu i√ßin d√ºzeltilmi≈ü\n",
        "            for v in os.listdir(subfolder_path) if v.endswith(('.mp4', '.avi', '.mov'))\n",
        "        ]\n",
        "        for video in videos:\n",
        "            data.append((video, \"normal\", 0, -1))  # Binary label = 0, Multi label = -1 (yok)\n",
        "\n",
        "# ‚úÖ Anormal videolar (Binary = 1, Multi = class index)\n",
        "for subfolder in abnormal_classes:\n",
        "    subfolder_path = os.path.normpath(os.path.join(abnormal_path, subfolder))\n",
        "\n",
        "    if os.path.isdir(subfolder_path):\n",
        "        # Rek√ºrsif olarak t√ºm alt klas√∂rleri gezer\n",
        "        for root, dirs, files in os.walk(subfolder_path):\n",
        "            for file in files:\n",
        "                if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "                    video_path = os.path.normpath(os.path.join(root, file)).replace(\"\\\\\", \"/\")\n",
        "                    # Burada 'subfolder' en √ºst d√ºzey klas√∂r adƒ±nƒ± temsil eder;\n",
        "                    # eƒüer alt klas√∂rlere g√∂re farklƒ± etiket atamak isterseniz, root veya dosya adƒ±nƒ± parse edebilirsiniz.\n",
        "                    data.append((video_path, subfolder, 1, class_to_index.get(subfolder, -1)))\n",
        "\n",
        "# üî• DataFrame olu≈ütur\n",
        "df = pd.DataFrame(data, columns=[\"video_path\", \"category\", \"binary_label\", \"multi_label\"])\n"
      ],
      "id": "8be6ceab98346e92",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T11:46:22.392226Z",
          "start_time": "2025-02-22T11:46:22.382103Z"
        },
        "id": "ffc00782cc50e573"
      },
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2,stratify=df[\"binary_label\"],random_state=42)\n",
        "\n",
        "print(f\"Train set {len(train_df)} videos\")\n",
        "print(f\"Test set {len(test_df)} videos\")"
      ],
      "id": "ffc00782cc50e573",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# **CUDA Kullanƒ±mƒ± ƒ∞√ßin Ayarla**\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def compute_optical_flow_torch(video_path, num_segments=3, segment_length=32):\n",
        "    \"\"\"CUDA ile PyTorch tabanlƒ± Optical Flow hesaplama.\"\"\"\n",
        "\n",
        "    vr = torchvision.io.VideoReader(video_path, \"video\")\n",
        "    vr.set_current_stream(\"video\")\n",
        "\n",
        "    frames = []\n",
        "    movement_scores = []\n",
        "\n",
        "    prev_frame = None  # ƒ∞lk kareyi saklamak i√ßin None olarak ba≈ülat\n",
        "\n",
        "    # üé• Video'dan frame'leri oku\n",
        "    for frame in vr:\n",
        "        frame = frame['data'].to(device).float() / 255.0  # **GPU'ya ta≈üƒ±**\n",
        "        frame = TF.resize(frame, (192, 192))  # **Boyutlandƒ±r**\n",
        "\n",
        "        gray = torch.mean(frame, dim=0, keepdim=True)  # **RGB'yi griye √ßevir**\n",
        "\n",
        "        if prev_frame is not None:\n",
        "            # **Optical Flow hesaplamak i√ßin √ßer√ßeveler arasƒ±ndaki fark**\n",
        "            flow = gray - prev_frame\n",
        "            movement = torch.sum(torch.abs(flow))  # **Hareket miktarƒ±nƒ± hesapla**\n",
        "            movement_scores.append(movement.item())\n",
        "\n",
        "        prev_frame = gray  # √ñnceki kareyi g√ºncelle\n",
        "        frames.append(frame)\n",
        "\n",
        "        # OOM hatasƒ± √∂nleme: Belleƒüi d√ºzenli temizle\n",
        "        if len(frames) > segment_length:\n",
        "            frames.pop(0)  # Eski kareleri √ßƒ±kararak bellek y√ºk√ºn√º azalt\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # **En hareketli b√∂l√ºmleri se√ß**\n",
        "    sorted_indices = torch.tensor(movement_scores).argsort(descending=True)[:num_segments]\n",
        "\n",
        "    return sorted_indices.cpu().tolist()  # GPU'dan √ßƒ±kar ve liste olarak d√∂n\n",
        "\n",
        "# üìå **T√ºm dataset i√ßin hesapla ve CSV'ye kaydet**\n",
        "dataset_folder = \"/content/sample_data/dataset/Videos/Videos/abnormal\"\n",
        "output_csv = \"/content/sample_data/optical_flow_segments.csv\"\n",
        "\n",
        "video_files = []\n",
        "for root, dirs, files in os.walk(dataset_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(('.mp4', '.avi', '.mov')):\n",
        "            video_path = os.path.normpath(os.path.join(root, file)).replace(\"\\\\\", \"/\")\n",
        "            video_files.append(video_path)\n",
        "\n",
        "# üöÄ **ƒ∞lerleme √ßubuƒüu ekleyerek i≈ülemi takip et**\n",
        "data = []\n",
        "for video_path in tqdm(video_files, desc=\"Processing Videos\", unit=\"video\"):\n",
        "    segments = compute_optical_flow_torch(video_path)\n",
        "    data.append({\"video_path\": video_path, \"top_segments\": segments})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"‚úÖ Optical Flow hesaplamasƒ± tamamlandƒ± ve {output_csv} dosyasƒ±na kaydedildi!\")"
      ],
      "metadata": {
        "id": "sOc8Qm8MLzeJ",
        "outputId": "67394801-721e-4da0-f41a-05c600b144ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sOc8Qm8MLzeJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2408/2408 [38:12<00:00,  1.05video/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Optical Flow hesaplamasƒ± tamamlandƒ± ve /content/sample_data/optical_flow_segments.csv dosyasƒ±na kaydedildi!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Augmentation\n",
        "\n",
        "import torchvision.transforms as T\n",
        "\n",
        "frame_transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=(-10, 10)),\n",
        "    T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
        "    T.RandomApply([T.GaussianBlur(3)], p=0.3),\n",
        "    T.RandomApply([T.RandomGrayscale(p=0.2)], p=0.2),\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "nTB7UP8sJxAq"
      },
      "id": "nTB7UP8sJxAq",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def frame_dropout(frames, p=0.2):\n",
        "    \"\"\"Belirli bir ihtimalle rastgele kareleri d√º≈ü√ºr.\"\"\"\n",
        "    keep_mask = torch.rand(len(frames)) > p\n",
        "    return frames[keep_mask] if keep_mask.sum() > 0 else frames\n",
        "\n",
        "def speed_perturbation(frames, min_speed=0.8, max_speed=1.2):\n",
        "    \"\"\"Videoyu rastgele hƒ±zlandƒ±r veya yava≈ülat.\"\"\"\n",
        "    speed_factor = torch.FloatTensor(1).uniform_(min_speed, max_speed).item()\n",
        "    indices = torch.linspace(0, len(frames)-1, int(len(frames) * speed_factor)).long()\n",
        "    return frames[indices]"
      ],
      "metadata": {
        "id": "Ltf1G3_QJ6WJ"
      },
      "id": "Ltf1G3_QJ6WJ",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixes frame count after applying frame dropout at augmentation\n",
        "\n",
        "def fix_frame_count(frames, target_num_frames):\n",
        "    \"\"\"Frame sayƒ±sƒ±nƒ± sabitler: Eƒüer fazla ise kƒ±rpar, eksikse tekrarlar.\"\"\"\n",
        "    num_frames = frames.shape[0]  # Frame sayƒ±sƒ±nƒ± al\n",
        "\n",
        "    if num_frames > target_num_frames:\n",
        "        frames = frames[:target_num_frames]  # Fazla olanlarƒ± kƒ±rp\n",
        "    elif num_frames < target_num_frames:\n",
        "        repeat_frames = target_num_frames - num_frames\n",
        "        last_frame = frames[-1].unsqueeze(0).repeat(repeat_frames, 1, 1, 1)  # Eksik frame'leri tekrar et\n",
        "        frames = torch.cat([frames, last_frame], dim=0)\n",
        "\n",
        "    return frames  # Her durumda (T, C, H, W) d√∂ner"
      ],
      "metadata": {
        "id": "Mc-L1wrgNvlu"
      },
      "id": "Mc-L1wrgNvlu",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T11:46:24.711059Z",
          "start_time": "2025-02-22T11:46:22.415593Z"
        },
        "id": "e0462ad59a40b35f"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, df, optical_flow_csv, num_frames=8, transform=None, resize_size=(192, 192), mode= \"train\"):\n",
        "        self.df = df\n",
        "        self.optical_flow_data = pd.read_csv(optical_flow_csv)  # üìå Optical Flow verileri\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform\n",
        "        self.resize_size = resize_size\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.df.iloc[idx][\"video_path\"]\n",
        "        binary_label = self.df.iloc[idx][\"binary_label\"]\n",
        "        multi_label = self.df.iloc[idx][\"multi_label\"]\n",
        "\n",
        "        if binary_label == 1:  # **Anormal Video**\n",
        "            # üìå **√ñnceden hesaplanan en hareketli segmentleri al**\n",
        "            segments = eval(self.optical_flow_data[self.optical_flow_data[\"video_path\"] == video_path][\"top_segments\"].values[0])\n",
        "\n",
        "            # üìå **Rastgele bir hareketli segment se√ß**\n",
        "            chosen_segment = random.choice(segments)\n",
        "\n",
        "            # üìå **Se√ßili segmentin frame'lerini al**\n",
        "            frames = self.extract_frames(video_path, chosen_segment)\n",
        "\n",
        "        else:  # **Normal Video**\n",
        "            frames = self.extract_uniform_frames(video_path, self.num_frames)\n",
        "\n",
        "        frames = fix_frame_count(frames, self.num_frames)\n",
        "\n",
        "        #Eƒüer train setindeysek augmentation uygula\n",
        "        if self.mode == \"train\":\n",
        "            frames = frame_dropout(frames, p=0.1)\n",
        "            frames = speed_perturbation(frames, min_speed=0.8, max_speed=1.2)\n",
        "            frames = torch.stack([frame_transform(frame) for frame in frames])\n",
        "            frames = fix_frame_count(frames,self.num_frames)\n",
        "\n",
        "\n",
        "        # **üéØ Gƒ∞Rƒ∞≈û FORMATINI D√úZELTME**\n",
        "        frames = frames.permute(1, 0, 2, 3).contiguous()  # (T, C, H, W) ‚Üí (C, T, H, W)\n",
        "\n",
        "        # Eƒüer transform varsa uygula\n",
        "        if self.transform:\n",
        "            frames = self.transform(frames)\n",
        "\n",
        "        return frames, torch.tensor(binary_label, dtype=torch.float32), torch.tensor(multi_label, dtype=torch.long)\n",
        "\n",
        "    def extract_frames(self, video_path, segment_index):\n",
        "        \"\"\"Anormal videolar i√ßin Optical Flow'a g√∂re se√ßilen segmentten frame'leri √ßƒ±karƒ±r.\"\"\"\n",
        "        vr = torchvision.io.VideoReader(video_path, \"video\")\n",
        "        vr.set_current_stream(\"video\")\n",
        "\n",
        "        frames = []\n",
        "        frame_count = 0\n",
        "        start_frame = segment_index  # üìå Se√ßilen hareketli segmentin ba≈ülangƒ±cƒ±\n",
        "        end_frame = start_frame + self.num_frames\n",
        "\n",
        "        for i, frame in enumerate(vr):\n",
        "            if i < start_frame:\n",
        "                continue  # üìå Se√ßilen segmentin ba≈ülangƒ±cƒ±na kadar frame'leri atla\n",
        "\n",
        "            if i >= end_frame:\n",
        "                break  # üìå Se√ßilen segment tamamlandƒ±ktan sonra √ßƒ±k\n",
        "\n",
        "            frame = frame['data'].float() / 255.0  # Normalize (C, H, W)\n",
        "            frame = F.interpolate(frame.unsqueeze(0), size=self.resize_size, mode=\"bilinear\", align_corners=False)\n",
        "            frames.append(frame.squeeze(0))\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        return torch.stack(frames)  # (T, C, H, W) olarak d√∂nd√ºr\n",
        "\n",
        "    def extract_uniform_frames(self, video_path, num_frames):\n",
        "        \"\"\"Normal videolar i√ßin e≈üit aralƒ±klarla frame se√ßer.\"\"\"\n",
        "        vr = torchvision.io.VideoReader(video_path, \"video\")\n",
        "        vr.set_current_stream(\"video\")\n",
        "\n",
        "        frames = []\n",
        "        for frame in vr:\n",
        "            frame = frame['data'].float() / 255.0  # Normalize (C, H, W)\n",
        "            frame = F.interpolate(frame.unsqueeze(0), size=self.resize_size, mode=\"bilinear\", align_corners=False)\n",
        "            frames.append(frame.squeeze(0))\n",
        "\n",
        "        total_frames = len(frames)\n",
        "\n",
        "        # Eƒüer video kƒ±sa ise, son frame ile doldur\n",
        "        if total_frames < num_frames:\n",
        "            while len(frames) < num_frames:\n",
        "                frames.append(frames[-1].clone())\n",
        "        else:\n",
        "            # üìå **Frame'leri e≈üit aralƒ±klarla se√ß**\n",
        "            index_list = torch.linspace(0, total_frames - 1, num_frames).long()\n",
        "            frames = [frames[i] for i in index_list]\n",
        "\n",
        "        return torch.stack(frames)  # (T, C, H, W)"
      ],
      "id": "e0462ad59a40b35f",
      "outputs": [],
      "execution_count": 81
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T11:46:24.722477Z",
          "start_time": "2025-02-22T11:46:24.711059Z"
        },
        "id": "22ce79d5c294f2c6",
        "outputId": "7565513a-752c-48ed-e0b3-cb5172be02dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset = VideoDataset(train_df, \"/content/sample_data/optical_flow_segments.csv\", num_frames=48, mode=\"train\")\n",
        "test_dataset = VideoDataset(test_df, \"/content/sample_data/optical_flow_segments.csv\", num_frames=48, mode=\"test\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(f\"Train loader {len(train_loader)} batches\")\n",
        "print(f\"Test loader {len(test_loader)} batches\")"
      ],
      "id": "22ce79d5c294f2c6",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader 163 batches\n",
            "Test loader 41 batches\n"
          ]
        }
      ],
      "execution_count": 86
    },
    {
      "metadata": {
        "id": "49f69497b15dc01f"
      },
      "cell_type": "markdown",
      "source": [
        "X3D modelimizin output layerini t√ºm classlarƒ±mƒ±zƒ± kapsayacak ≈üekilde arttƒ±rmamƒ±z lazƒ±m\n",
        "Son katman ise ResNetBasicHead altƒ±nda proj olarak tanƒ±mlanmƒ±≈ü."
      ],
      "id": "49f69497b15dc01f"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T11:46:26.156357Z",
          "start_time": "2025-02-22T11:46:24.736007Z"
        },
        "id": "eb2055b4174b845f",
        "outputId": "df1adcaa-f6b8-4cab-939b-c95268daa4a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "# ‚úÖ X3D Modelini Y√ºkleme\n",
        "model_name = \"x3d_s\"\n",
        "base_model = torch.hub.load('facebookresearch/pytorchvideo', model_name, pretrained=True)\n",
        "\n",
        "# ‚úÖ **√áƒ±kƒ±≈ü Katmanlarƒ±nƒ± G√ºncelle**\n",
        "in_features = base_model.blocks[-1].proj.out_features  # ‚úÖ **Doƒüru √ßƒ±kƒ±≈üƒ± al!** (400 olmalƒ±)\n",
        "\n",
        "binary_head = nn.Linear(2048, 1)  # Binary sƒ±nƒ±flandƒ±rma i√ßin Sigmoid\n",
        "multi_head = nn.Linear(2048, len(train_df[\"multi_label\"].unique()))  # Multi-class sƒ±nƒ±flandƒ±rma\n",
        "\n",
        "# ‚úÖ **MultiTaskX3D Modeli**\n",
        "class MultiTaskX3D(nn.Module):\n",
        "    def __init__(self, base_model, binary_head, multi_head):\n",
        "        super(MultiTaskX3D, self).__init__()\n",
        "\n",
        "        # **Feature Extractor - Son bloƒüun i√ßindeki ProjectedPool eklendi!**\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            *base_model.blocks[:-1],\n",
        "            base_model.blocks[-1].pool  # ‚úÖ **Burada 400'e d√∂n√º≈ü√ºm saƒülandƒ±!**\n",
        "        )\n",
        "\n",
        "        # **Global Average Pooling - T√ºm boyutlarƒ± 1x1x1'e d√º≈ü√ºr√ºyor!**\n",
        "        self.gap = nn.AdaptiveAvgPool3d(1)\n",
        "\n",
        "        # **Flatten**\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # **√áƒ±kƒ±≈ü Katmanlarƒ±**\n",
        "        self.binary_head = binary_head\n",
        "        self.binary_activation = nn.Sigmoid()\n",
        "\n",
        "        self.multi_head = multi_head\n",
        "        self.multi_activation = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"üöÄ Model Giri≈üi: {x.shape}\")\n",
        "\n",
        "        x = self.feature_extractor(x)\n",
        "        #print(f\"üü° Feature Extractor √áƒ±kƒ±≈üƒ±: {x.shape}\")  # **(16, 400, 1, 1, 1) olmalƒ±!**\n",
        "\n",
        "        x = self.gap(x)  # ‚úÖ **GAP ile (16, 400, 1, 1, 1)**\n",
        "        #print(f\"üîµ GAP Sonrasƒ± Boyut: {x.shape}\")\n",
        "\n",
        "        x = self.flatten(x)  # ‚úÖ **Artƒ±k (16, 400) olmasƒ± lazƒ±m!**\n",
        "        #print(f\"üü¢ Flatten Sonrasƒ± Boyut: {x.shape}\")\n",
        "\n",
        "        binary_out = self.binary_head(x)  # Binary sƒ±nƒ±flandƒ±rma\n",
        "        multi_out = self.multi_activation(self.multi_head(x))  # Multi-class sƒ±nƒ±flandƒ±rma\n",
        "\n",
        "        return binary_out, multi_out\n",
        "\n",
        "# ‚úÖ Modeli ba≈ülat\n",
        "model = MultiTaskX3D(base_model, binary_head, multi_head)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"üöÄ Model ba≈üarƒ±yla y√ºklendi! √áalƒ±≈ütƒ±rƒ±lan cihaz: {device}\")"
      ],
      "id": "eb2055b4174b845f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Model ba≈üarƒ±yla y√ºklendi! √áalƒ±≈ütƒ±rƒ±lan cihaz: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
          ]
        }
      ],
      "execution_count": 87
    },
    {
      "metadata": {
        "id": "cf0f8ab8e20aa3b7"
      },
      "cell_type": "markdown",
      "source": [
        "import torch\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "dummy_input = torch.randn(1, 3, 16, 160, 160)\n",
        "\n",
        "torch.onnx.export(model, dummy_input, \"x3d_xs.onnx\", opset_version=11\n",
        "                  ,do_constant_folding=True, input_names=[\"input\"], output_names=[\"output\"])"
      ],
      "id": "cf0f8ab8e20aa3b7"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self,inputs,targets):\n",
        "        ce_loss = F.cross_entropy(inputs,targets,reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
        "        return focal_loss.mean()"
      ],
      "metadata": {
        "id": "pZxmxvCalU2T"
      },
      "id": "pZxmxvCalU2T",
      "execution_count": 88,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-02-22T12:20:55.814874Z",
          "start_time": "2025-02-22T11:46:26.188132Z"
        },
        "id": "9b297db9945016d9",
        "outputId": "4a2af58b-df39-454f-a612-eea01cd36ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "\n",
        "epoch_file_location = \"/content/drive/MyDrive/modelepochsaves\"\n",
        "\n",
        "# üéØ Loss & Optimizer\n",
        "criterion_binary = nn.BCEWithLogitsLoss()  # Binary classification i√ßin\n",
        "criterion_multi = FocalLoss()  # Multi-class classification i√ßin\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-2)  # Weight decay ile overfitting azaltƒ±lƒ±r\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# ‚ö° AMP (Automatic Mixed Precision)\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# üõë Early Stopping Parametreleri\n",
        "patience = 3  # Ka√ß epoch boyunca iyile≈üme olmazsa durdursun\n",
        "min_delta = 0.01  # ƒ∞yile≈üme i√ßin gereken minimum fark\n",
        "best_loss = float(\"inf\")  # En iyi validation loss ba≈üta sonsuz olarak ayarlanƒ±r\n",
        "counter = 0  # ƒ∞yile≈üme olmayan epoch sayƒ±sƒ±\n",
        "accuracy_threshold = 85.0  # Accuracy'nin ge√ßmesi gereken e≈üik\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "for epoch in range(40):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for videos, binary_labels, multi_labels in train_loader:\n",
        "        videos, binary_labels, multi_labels = (\n",
        "            videos.to(device),\n",
        "            binary_labels.to(device).float(),\n",
        "            multi_labels.to(device),\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device):\n",
        "            binary_out, multi_out = model(videos)\n",
        "\n",
        "            # üü¢ Binary Classification Loss (Sigmoid)\n",
        "            binary_loss = criterion_binary(binary_out.squeeze(), binary_labels)\n",
        "\n",
        "            # üî¥ Multi-Class Classification Loss (Softmax) ‚Üí Sadece anormal olanlara uygula\n",
        "            mask = (binary_labels == 1)\n",
        "            if mask.sum() > 0:\n",
        "                multi_loss = criterion_multi(multi_out[mask], multi_labels[mask])\n",
        "            else:\n",
        "                multi_loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "            # ‚úÖ Toplam loss hesapla (Binary + Multi)\n",
        "            total_loss = binary_loss + multi_loss\n",
        "\n",
        "        scaler.scale(total_loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += total_loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "\n",
        "    # üìå VALIDATION A≈ûAMASI\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_binary = 0\n",
        "    correct_multi = 0\n",
        "    total_binary = 0\n",
        "    total_multi = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for videos, binary_labels, multi_labels in test_loader:\n",
        "            videos, binary_labels, multi_labels = (\n",
        "                videos.to(device),\n",
        "                binary_labels.to(device).float(),\n",
        "                multi_labels.to(device),\n",
        "            )\n",
        "\n",
        "            binary_out, multi_out = model(videos)\n",
        "\n",
        "            # ‚úÖ Binary Accuracy (Normal mi, Anormal mi?)\n",
        "            pred_binary = (binary_out.squeeze() > 0.5).long()\n",
        "            correct_binary += (pred_binary == binary_labels).sum().item()\n",
        "            total_binary += binary_labels.size(0)\n",
        "\n",
        "            # ‚úÖ Multi-Class Accuracy (Eƒüer anormalse hangi sƒ±nƒ±f?)\n",
        "            mask = (binary_labels == 1)\n",
        "            if mask.sum() > 0:\n",
        "                pred_multi = torch.argmax(multi_out[mask], dim=1)\n",
        "                correct_multi += (pred_multi == multi_labels[mask]).sum().item()\n",
        "                total_multi += multi_labels[mask].size(0)\n",
        "\n",
        "            # ‚úÖ Validation loss hesapla\n",
        "            loss = criterion_binary(binary_out.squeeze(), binary_labels)\n",
        "            if total_multi > 0:\n",
        "                loss += criterion_multi(multi_out[mask], multi_labels[mask])\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    binary_acc = 100 * correct_binary / total_binary\n",
        "    multi_acc = 100 * correct_multi / total_multi if total_multi > 0 else 0\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} \"\n",
        "        f\"| Binary Acc: {binary_acc:.2f}% | Multi Acc: {multi_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "    # ‚úÖ **Early Stopping (Binary Accuracy 85%'i ge√ßmeli)**\n",
        "    if binary_acc >= accuracy_threshold:\n",
        "        if best_loss - val_loss > min_delta:\n",
        "            best_loss = val_loss\n",
        "            counter = 0\n",
        "            torch.save(model.state_dict(), f\"{epoch_file_location}/best_model_epoch_{epoch+1}.pth\")\n",
        "            print(f\"‚úÖ Model kaydedildi: best_model_epoch_{epoch+1}.pth\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"‚ùó Early stopping counter: {counter}/{patience}\")\n",
        "\n",
        "        if counter >= patience:\n",
        "            print(\"‚èπÔ∏è Early stopping triggered. Training stopped.\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"üö® Accuracy {binary_acc:.2f}% < {accuracy_threshold}%, early stopping devre dƒ±≈üƒ±!\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            },\n",
        "            f\"{epoch_file_location}/checkpoint_{epoch}.pth\",\n",
        "        )\n",
        "        print(f\"Model {epoch + 1} saved to checkpoint_{epoch + 1}.pth\")\n",
        "\n",
        "print(\"‚úÖ Eƒüitim tamamlandƒ±!\")"
      ],
      "id": "9b297db9945016d9",
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 5531 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 343.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-310adc6c5613>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mbinary_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# üü¢ Binary Classification Loss (Sigmoid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-c6709afd6de9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print(f\"üöÄ Model Giri≈üi: {x.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print(f\"üü° Feature Extractor √áƒ±kƒ±≈üƒ±: {x.shape}\")  # **(16, 400, 1, 1, 1) olmalƒ±!**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1398\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1_conv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m             \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_b\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;31m# Branch2c, for example 1x1x1, BN.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/layers/swish.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSwishFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/layers/swish.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 5531 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 343.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "execution_count": 89
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}